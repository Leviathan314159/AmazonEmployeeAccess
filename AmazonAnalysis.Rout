WARNING: unknown option '--no-resume'


R version 4.3.1 (2023-06-16) -- "Beagle Scouts"
Copyright (C) 2023 The R Foundation for Statistical Computing
Platform: x86_64-pc-linux-gnu (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

  Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> # This file is for the Amazon Employee Access Kaggle competition.
> # This is the primary analysis file
> 
> # Libraries -----------------------------------------------
> library(tidymodels)
── Attaching packages ────────────────────────────────────── tidymodels 1.1.1 ──
✔ broom        1.0.5     ✔ recipes      1.0.8
✔ dials        1.2.0     ✔ rsample      1.2.0
✔ dplyr        1.1.3     ✔ tibble       3.2.1
✔ ggplot2      3.4.3     ✔ tidyr        1.3.0
✔ infer        1.0.5     ✔ tune         1.1.2
✔ modeldata    1.2.0     ✔ workflows    1.1.3
✔ parsnip      1.1.1     ✔ workflowsets 1.0.1
✔ purrr        1.0.2     ✔ yardstick    1.2.0
── Conflicts ───────────────────────────────────────── tidymodels_conflicts() ──
✖ purrr::discard() masks scales::discard()
✖ dplyr::filter()  masks stats::filter()
✖ dplyr::lag()     masks stats::lag()
✖ recipes::step()  masks stats::step()
• Use tidymodels_prefer() to resolve common conflicts.
> library(vroom)

Attaching package: ‘vroom’

The following object is masked from ‘package:yardstick’:

    spec

The following object is masked from ‘package:scales’:

    col_factor

> # library(ggmosaic)
> library(tidyverse)
── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──
✔ forcats   1.0.0     ✔ readr     2.1.4
✔ lubridate 1.9.3     ✔ stringr   1.5.0
── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
✖ readr::col_character()   masks vroom::col_character()
✖ readr::col_date()        masks vroom::col_date()
✖ readr::col_datetime()    masks vroom::col_datetime()
✖ readr::col_double()      masks vroom::col_double()
✖ readr::col_factor()      masks vroom::col_factor(), scales::col_factor()
✖ readr::col_guess()       masks vroom::col_guess()
✖ readr::col_integer()     masks vroom::col_integer()
✖ readr::col_logical()     masks vroom::col_logical()
✖ readr::col_number()      masks vroom::col_number()
✖ readr::col_skip()        masks vroom::col_skip()
✖ readr::col_time()        masks vroom::col_time()
✖ readr::cols()            masks vroom::cols()
✖ readr::date_names_lang() masks vroom::date_names_lang()
✖ readr::default_locale()  masks vroom::default_locale()
✖ purrr::discard()         masks scales::discard()
✖ dplyr::filter()          masks stats::filter()
✖ stringr::fixed()         masks recipes::fixed()
✖ readr::fwf_cols()        masks vroom::fwf_cols()
✖ readr::fwf_empty()       masks vroom::fwf_empty()
✖ readr::fwf_positions()   masks vroom::fwf_positions()
✖ readr::fwf_widths()      masks vroom::fwf_widths()
✖ dplyr::lag()             masks stats::lag()
✖ readr::locale()          masks vroom::locale()
✖ readr::output_column()   masks vroom::output_column()
✖ readr::problems()        masks vroom::problems()
✖ readr::spec()            masks vroom::spec(), yardstick::spec()
ℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors
> library(embed)
> library(ranger)
> 
> # Read in the data ------------------------------------
> base_folder <- "AmazonEmployeeAccess/"
> access_train <- vroom(paste0(base_folder, "train.csv"))
Rows: 32769 Columns: 10
── Column specification ────────────────────────────────────────────────────────
Delimiter: ","
dbl (10): ACTION, RESOURCE, MGR_ID, ROLE_ROLLUP_1, ROLE_ROLLUP_2, ROLE_DEPTN...

ℹ Use `spec()` to retrieve the full column specification for this data.
ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.
> access_test <- vroom(paste0(base_folder, "test.csv"))
Rows: 58921 Columns: 10
── Column specification ────────────────────────────────────────────────────────
Delimiter: ","
dbl (10): id, RESOURCE, MGR_ID, ROLE_ROLLUP_1, ROLE_ROLLUP_2, ROLE_DEPTNAME,...

ℹ Use `spec()` to retrieve the full column specification for this data.
ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.
> 
> # glimpse(access_train)
> # glimpse(access_test)
> 
> # Exploratory Data Analysis --------------------------
> # # Create dataset for exploration
> # access_explore <- access_train
> # access_explore$ACTION <- factor(access_explore$ACTION)
> # glimpse(access_explore)
> # 
> # # Write a function to assign any category with less than p% as "Other"
> # condense_as_other <- function(this_data, x, p){
> #   # This function condenses the column in this_data specified by "x" according to percent p.
> #   # Inputs:
> #   # this_data - a tibble
> #   # x - a string containing the name of the column you want to reference
> #   # p - a decimal representing the percent required to label a category as "other"
> #   #     (e.g. if p = 0.01, then any category with fewer than 1% of the total observations gets labeled as "Other")
> #   n <- length(this_data[[x]])
> #   resource_freq <- 1:n
> #   for (i in 1:n) {
> #     resource_freq[i] = sum(this_data[[x]][i] == this_data[[x]])/n
> #     if (resource_freq[i] < p) {
> #       this_data[[x]][i] = "Other"
> #     }
> #   }
> #   
> #   # Return the data
> #   this_data
> # }
> # 
> # # Condense all predictors
> # for (this_name in names(access_explore)[2:length(names(access_explore))]) {
> #   access_explore <- condense_as_other(access_explore, this_name, 0.01)
> # }
> # 
> # access_explore |> group_by(RESOURCE) |> summarize()
> # access_explore |> group_by(MGR_ID) |> summarize()
> # access_explore |> group_by(ROLE_ROLLUP_1) |> summarize()
> # access_explore |> group_by(ROLE_ROLLUP_2) |> summarize()
> # access_explore |> group_by(ROLE_DEPTNAME) |> summarize()
> # access_explore |> group_by(ROLE_TITLE) |> summarize()
> # access_explore |> group_by(ROLE_FAMILY_DESC) |> summarize()
> # access_explore |> group_by(ROLE_FAMILY) |> summarize()
> # access_explore |> group_by(ROLE_CODE) |> summarize()
> # 
> # # Plot access ACTION for various condensed predictors
> # # ROLE_DEPTNAME
> # ggplot(data = access_explore) + 
> #   geom_mosaic(mapping = aes(x = product(ROLE_DEPTNAME), fill = ACTION))
> # ggsave(paste0(base_folder, "Department Name Access Plot.png"))
> # # ROLE_TITLE
> # ggplot(data = access_explore) + 
> #   geom_mosaic(mapping = aes(x = product(ROLE_TITLE), fill = ACTION))
> # ggsave(paste0(base_folder, "Role Title Access Plot.png"))
> 
> # Recipes ---------------------------------------
> 
> # Make sure the response variable is categorical
> access_train$ACTION <- as.factor(access_train$ACTION)
> 
> # Set the threshold percent to use for making a category "other"
> threshold_percent <- 0.001
> 
> # Apply a recipe that condenses infrequent data values into "other" categories
> access_recipe <- recipe(ACTION ~ ., data = access_train) |> 
+   step_mutate_at(all_numeric_predictors(), fn = factor) |> # turns all numeric features into factors
+   step_other(all_nominal_predictors(), threshold = threshold_percent) |> # condenses categorical values that are less than 1% into an "other" category
+   step_dummy(all_nominal_predictors()) # encode to dummy variables
> 
> # prepped_access_recipe <- prep(access_recipe)
> # baked_access <- bake(prepped_access_recipe, new_data = access_train)
> # glimpse(baked_access) # Check how many columns there are; should be 112
> 
> 
> # Recipe for penalized logsitic regression
> penalized_logistic_recipe <- recipe(ACTION ~ ., data = access_train) |> 
+   step_mutate_at(all_numeric_predictors(), fn = factor) |> 
+   step_other(all_nominal_predictors(), threshold = threshold_percent) |> 
+   step_lencode_mixed(all_nominal_predictors(), outcome = vars(ACTION))
> 
> # Recipe for random forest
> tree_recipe <- recipe(ACTION ~ ., data = access_train) |> 
+   step_mutate_at(all_numeric_predictors(), fn = factor) |> 
+   step_other(all_nominal_predictors(), threshold = threshold_percent) # |> 
>   # step_lencode_mixed(all_nominal_predictors(), outcome = vars(ACTION))
> 
> # Logistic Regression Model -----------------------
> # logistic_mod <- logistic_reg() |> set_engine("glm")
> # 
> # logistic_amazon_wf <- workflow() |> 
> #   add_recipe(access_recipe) |> 
> #   add_model(logistic_mod) |> 
> #   fit(data = access_train)
> # 
> # logistic_amazon_pred <- predict(logistic_amazon_wf, 
> #                                 new_data = access_test,
> #                                 type = "prob")
> # logistic_amazon_pred
> # logistic_amazon_export <- data.frame("id" = 1:length(logistic_amazon_pred$.pred_1),
> #                                      "Action" = logistic_amazon_pred$.pred_1)
> # 
> # Penalized Logistic Regression Model -----------------------
> # penalized_logistic_mod <- logistic_reg(mixture = tune(), penalty = tune()) |> 
> #   set_engine("glmnet")
> # 
> # penalized_amazon_wf <- workflow () |>
> #   add_recipe(penalized_logistic_recipe) |> 
> #   add_model(penalized_logistic_mod)
> # 
> # # Set the tuning grid
> # amazon_logistic_tuning_grid <- grid_regular(penalty(),
> #                                             mixture(),
> #                                             levels = 5)
> # 
> # # Set up the CV
> # penalized_amazon_folds <- vfold_cv(access_train, v = 10, repeats = 1)
> # 
> # # Run the CV
> # penalized_CV_results <- penalized_amazon_wf |> 
> #   tune_grid(resamples = penalized_amazon_folds,
> #             grid = amazon_logistic_tuning_grid,
> #             metrics = metric_set(roc_auc)) #, f_meas, sens, recall, spec,
> #                                  # precision, accuracy))
> # 
> # # Find out the best tuning parameters
> # best_tune <- penalized_CV_results |> select_best("roc_auc")
> # best_tune
> # 
> # # Use the best tuning parameters for the model
> # final_penalized_wf <- penalized_amazon_wf |> 
> #   finalize_workflow(best_tune) |> 
> #   fit(data = access_train)
> # 
> # # Predictions
> # penalized_logistic_preds <- final_penalized_wf |> 
> #   predict(new_data = access_test, type = "prob")
> # 
> # # Prepare export
> # penalized_export <- data.frame("id" = 1:length(penalized_logistic_preds$.pred_1),
> #                                "Action" = penalized_logistic_preds$.pred_1)
> # 
> # 
> # Random Forest (Classification) -----------------------------
> forest_amazon <- rand_forest(mtry = tune(),
+                              min_n = tune(),
+                              trees = 500) |> 
+   set_engine("ranger") |> 
+   set_mode("classification")
> 
> # Create a workflow using the model and recipe
> forest_amazon_wf <- workflow() |> 
+   add_model(forest_amazon) |> 
+   add_recipe(tree_recipe)
> 
> # Set up the grid with the tuning values
> forest_amazon_grid <- grid_regular(mtry(range = c(1, (length(access_train)-1))), min_n())
> 
> # Set up the K-fold CV
> forest_amazon_folds <- vfold_cv(data = access_train, v = 10, repeats = 1)
> 
> # Find best tuning parameters
> forest_cv_results <- forest_amazon_wf |> 
+   tune_grid(resamples = forest_amazon_folds,
+             grid = forest_amazon_grid,
+             metrics = metric_set(roc_auc))
> 
> coll> 
> collect_metrics(forest_cv_results) |> 
+   filter(.metric == "rmse") %>% 
+   ggplot(data = ., aes(x = mtry, y = mean, color = factor(min_n))) + 
+   geom_point()
> 
> # Finalize the workflow using the best tuning parameters and predict
> # The best parameters were mtry = 9 and min_n = 2
> 
> # Find out the best tuning parameters
> best_forest_tune <- forest_cv_results |> select_best("roc_auc")
> 
> # Use the best tuning parameters for the model
> forest_final_wf <- forest_amazon_wf |> 
+   finalize_workflow(best_forest_tune) |> 
+   fit(data = acces> 
> fore> 
> forest_amazon_predictions <- predict(forest_final_wf, new_data = access_test)
> forest_amazon_predictions
# A tibble: 58,921 × 1
   .pred_class
   <fct>      
 1 0          
 2 1          
 3 1          
 4 1          
 5 1          
 6 1          
 7 1          
 8 1          
 9 1          
10 1          
# ℹ 58,911 more rows
> 
> forest_export <- data.frame("id" = 1:length(forest_amazon_predictions$.pred_class),
+                                "Action" = forest_amazon_predictions$.pred_class)
> 
> # Write the data ---------------------------------
> # vroom_write(logistic_amazon_export, paste0(base_folder, "logistic.csv"), delim = ",")
> # vroom_write(penalized_export, paste0(base_folder, "penalized_logistic.csv"), delim = ",")
> vroom_write(forest_export, paste0(base_folder, "random_forest_classification.csv"), delim =",")
> 
> proc.time()
    user   system  elapsed 
1279.105   23.656 1203.476 
55 1195.442 
